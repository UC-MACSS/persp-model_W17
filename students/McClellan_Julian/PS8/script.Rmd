---
title: "Problem Set 8 | MACS 301"
author: "Julian McClellan"
date: "Due 3/6/17"
output:
  html_document: default
  pdf_document: 
    latex_engine: lualatex
---
```{r setup, echo = FALSE, include = FALSE}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(titanic)
library(pROC)
library(gbm)
library(ggdendro)
library(devtools)
library(rcfss)
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)
df.biden <- read_csv('data/biden.csv')
df.mhealth <- read_csv('data/mental_health.csv')
options(digits = 3)
theme_set(theme_minimal())
```

# Part 1: Sexy Joe Biden

#### 1. Split the data into a training set (70%) and a validation set (30%). *Be sure to set your seed prior to this part of your code to guarantee reproducibility of results.*

We utilize 70% of the data as training data, and the remaining 30% as testing data.

```{r split_biden, echo = TRUE}
set.seed(1234) # For reproducibility
biden.split <- resample_partition(df.biden, c(test = .3, train = .7))
```

***

#### 2. Fit a decision tree to the training data, with biden as the response variable and the other variables as predictors. Plot the tree and interpret the results. What is the test MSE?

```{r biden_tree0}
# Make tree model
tree.biden <- tree(biden ~ ., data = biden.split$train)

# plot tree
tree_data <- dendro_data(tree.biden)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = 'Decision Tree for Biden Score', 
       subtitle = 'Default controls, all predictors')

# Based off of resampling class notes
calc_mse <- function(model, data){
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

leaf_vals <- leaf_label(tree_data)$yval
test_mse <- calc_mse(tree.biden, biden.split$test)
```

Using the default tree controls to make the tree, interpretation is relatively easy. Indeed, the tree was built using only `dem` and `rep` as predictors, so let's interpret. Starting at the top If `dem > 0.5`, more plainly, if someone is a democrat (`dem = 1`), then the tree predicts a `biden` score of `r leaf_vals[3]`. Otherwise, if `dem < 0.5`, if someone is not a democrat, then we take the left branch of the tree to the next decision point. If, at this decision point, `rep < .05`, i.e. someone is not republican (an independent since they are neither `rep` or `dem` is `1`), then the tree predicts a `biden` score of `leaf_vals[2]`. Otherwise, if `rep > 0.5`, i.e. they are a republican, the tree predicts a `biden` score of `leaf_vals[3]`.  

Additionally, the test MSE appears to be `r test_mse`.

***

#### 3. Now fit another tree to the training data with some customized control options. Use cross-validation to determine the optimal level of tree complexity, plot the optimal tree, and interpret the results. Does pruning the tree improve the test MSE?

```{r biden_prune_tree}
tree.base <- tree(biden ~ . , data = biden.split$train, 
                     control = tree.control(nobs = nrow(biden.split$train),
                              mindev = 0))
base_test_mse <- calc_mse(tree.base, biden.split$test)

num_nodes <- 2:25
pruned_trees <- map(num_nodes, prune.tree, tree = tree.base, k = NULL)
test_mses <- map_dbl(pruned_trees, calc_mse, data = biden.split$test)

tree.opt <- pruned_trees[[which.min(test_mses)]]
opt_test_mse <- calc_mse(tree.opt, biden.split$test)

tibble(num_nodes = num_nodes, test_mse = test_mses) %>%
  ggplot(aes(x = num_nodes, y = test_mse)) +
  geom_line() + 
  labs(title = 'Test MSE for Different Numbers of Terminal Nodes',
       subtitle = '(Test MSE calculated on Test data defined in Step 1)',
       x = 'Terminal Nodes in Tree',
       y = 'Test MSE')
```

According to the graph, the minimum test MSE occurs at `r summary(tree.opt)$size` terminal nodes in the tree. Clearly, pruning the tree helps reduce the test MSE. Indeed, the original tree had `r summary(tree.base)$size` terminal nodes and a test MSE of `r base_test_mse`, and yet the optimal tree has only `r summary(tree.opt)$size` terminal nodes and a test MSE of `r opt_test_mse`.

Now let's plot the optimal tree.

```{r biden_prune_121}
# plot tree
tree_data <- dendro_data(pruned_trees[[which.min(test_mses)]], type = 'uniform') 
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data),
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data),
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = 'Optimal Decision Tree for Biden Score', 
       subtitle = sprintf('All Predictors | %d Terminal Nodes', summary(tree.opt)$size))

leaf_vals <- leaf_label(tree_data)$yval
```

While I won't interpret all 11 possible paths, I will, interpret one path. At the top, at the first decision node, if `dem < 0.5`, i.e. `dem` is 0, we go left to another decision node. If `rep < 0.5`, i.e. `rep` is 0, we go left again to another decision node. If `female < 0.5`, i.e. `female` is 0 (male), we go left again and arrive at a terminal node, where the tree predicts a `biden` score of `r leaf_vals[1]`.

***

#### 4. Use the bagging approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results.

```{r biden_bagging}
set.seed(1234) # Need reproducible bootstrap
(biden_bag <- randomForest(biden ~ female + age + educ + dem + rep, data = biden.split$train,
                           mtry = length(df.biden) - 1, importance = TRUE,
                           ntree = 500))

# Calculate bagged MSE
mse_bag <- calc_mse(biden_bag, biden.split$test)

# Get variable importance
var_import <- importance(biden_bag, type = 1)

# Variable importance measures
data_frame(var = rownames(importance(biden_bag)),
           MeanDecreaseGini = var_import[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_col() +
  coord_flip() +
  labs(title = "Predicting Biden Warmth Score",
       subtitle = "Bagging: 500 Trees",
       x = NULL,
       y = "% Average Decrease in Out of Bag MSE")
```

The test MSE for the bagging approach is `r mse_bag`. 

Looking at the variable importance graph above, our x-axis is the median % decrease in OOB MSE. Clearly, splitting on `dem` reduces the OOB MSE by the greatest margin, at an `r var_import[4]`% median decrease. Splitting on `rep` also provides a decent median reduction, with  a `r var_import[5]`%. Splitting on the other variables, however, do not help reduce OOB MSE by much. Indeed, splitting on `female` and `educ` actually *increases* the OOB MSE by median values of `r -1 * var_import[1]` and `r -1 * var_import[3]` respectively, while splitting on `age` only provides a median decrease of the OOB MSE by `r var_import[2]`%.

*** 

#### 5. Use the random forest approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

```{r biden_rforest}
set.seed(1234) # Need reproducible bootstrap
(biden_rforest <- randomForest(biden ~ female + age + educ + dem + rep, 
                               data = biden.split$train,
                              importance = TRUE,
                              ntree = 500))

# Calculate rforest MSE
mse_rforest <- calc_mse(biden_rforest, biden_split$test)

# Get variable importance
var_import <- importance(biden_rforest, type = 1)

# Variable importance measures
data_frame(var = rownames(importance(biden_rforest)),
           MeanDecreaseGini = var_import[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_col() +
  coord_flip() +
  labs(title = "Predicting Biden Warmth Score",
       subtitle = sprintf("Random Forest: 500 trees: %d randomly chosen predictors at each split", 
                          floor(sqrt(length(df.biden) - 1))),
       x = NULL,
       y = "% Median Decrease in Out of Bag MSE")
```

```{r rforest_bag_compare}
data_frame(var = rownames(importance(biden_rforest)),
           `Random forest` = importance(biden_rforest, type = 1)[,1]) %>%
  left_join(data_frame(var = rownames(importance(biden_bag)),
           Bagging = importance(biden_bag, type = 1)[,1])) %>%
  mutate(var = fct_reorder(var, `Random forest`, fun = median)) %>%
  {.} -> wut

wut %>%
  gather(model, gini, -var) %>%
  {.} -> wut

wut %>%
  ggplot(aes(var, gini)) +
  geom_col(aes(fill = model), position = 'dodge') +
    coord_flip() +
    labs(title = "Predicting Biden Warmth Score",
         subtitle = "Random Forest vs. Bagging",
         x = NULL,
         y = "Average % decrease in the OOB MSE",
         fill = "Method")
```
The random forest obtains a test MSE of `r mse_rforest`.

As we can see, utilizing a random forest changes our variable importance measures somewhat. Previously, `dem` had a large median decrease at `~80%`. now it only has a `r var_import[4]`% median decrease for OOB MSE. The other variables all have had their median decrease in OOB MSE increase to varying levels. Indeed, `female` now provides a higher median decrease in OOB MSE than `age` and `educ`. Also noteworthy is the fact that `female` and `educ` actually have median *decreases* in OOB MSE rather than increases as there was previously.  

Now let's take a look at the effect of `m`, the number of randomly chosen predictors chosen as split criteria as each decision node, on the test MSE for a random forest.  

```{r biden_rforest_m}
set.seed(1234) # Need reproducible bootstrap
forests <- data_frame(num_predictors = 1:5,
                        models = map(num_predictors, 
                                     ~ randomForest(biden ~ ., 
                                                    data = biden.split$train, 
                                                    ntree = 500, importance = TRUE, 
                                                    mtry = .)),
                        test_mse = map_dbl(models, calc_mse, data = biden.split$test))

forests %>%
  ggplot(aes(num_predictors, test_mse)) +
  geom_line() +
  labs(title = "Random Forest Test MSE",
       subtitle = "Random Forest: 500 trees",
       x = "Number of Predictors Randomly Chosen as Split Criteria at each Decision Node",
       y = "Test MSE")
```

Clearly, looking at the above graph, the number of predictors (`m`) we randomly select to serve as possible split criteria at each decision node has an effect on the test MSE. Indeed, in this case, we see a general trend that the lower `m` is, the lower our test MSE. However, we don't want to go too low and set `m = 1`, as the ideal value for `m` according to this graph is `2`.

***

#### 6. Use the boosting approach to analyze the data. What test MSE do you obtain? How does the value of the shrinkage parameter Î» influence the test MSE?


```{r biden_boost, warning = FALSE}
pred_trees <- 100
int_depth <- 4

biden.boost <- gbm(biden ~ ., 
                   data = biden.split$train, distribution = 'gaussian',
                   n.trees = 10000, interaction.depth = int_depth)

mse_boost <- mean((biden.split$test$data$biden - 
                     predict(biden.boost, newdata = biden.split$test, n.trees = pred_trees)) ^ 2)

num_trees_df <- data_frame(num_trees = seq(100, 10000, by = 100),
                           test_mse = map_dbl(num_trees, ~ mean((biden.split$test$data$biden - 
                                                             predict(biden.boost, 
                                                                     newdata = biden.split$test, 
                                                                     n.trees = .)) ^ 2)))

num_trees_df %>%
  ggplot(aes(num_trees, test_mse)) +
    geom_line() +
    labs(title = 'Number of trees used vs. Test MSE',
         subtitle = sprintf('Interaction depth = %d', int_depth),
         x = 'Number of Trees used in Boosting',
         y = 'Test MSE')
```

Utilizing the boosting approach to analyze the data, with `r pred_trees` trees, and an interaction depth of `r int_depth` I obtain a test MSE of `r mse_boost`. This is way higher than any of the previous test MSEs I obtained via bagging or with a random forest. Additionally, above we show a graph of the test MSE as a function of the number of trees used in Boosting. Oddly enough, the number of trees used seems to increase the test MSE! This is not what we expect. If you're grading this, take a look at my code, I'm pretty sure I didn't fuck it up.

Now let's see how the value of the shrinkage parameter $\lambda$ has on the test MSE by training several boosted models with the same number of trees, with different values of the shrinkage parameter.

```{r shrinkage}
shrink_df <- data_frame(shrinkage = seq(0.001, .1, length.out = 100),
           boosts = map(shrinkage, ~ gbm(biden ~ .,
                                         data = biden.split$train, distribution = 'gaussian',
                                         n.trees = 1000, interaction.depth = 3, shrinkage = .)),
           mse_boost = map_dbl(boosts, 
                           ~ mean((biden.split$test$data$biden - predict(., 
                                                                         n.trees = num_trees,
                                                                  newdata = biden.split$test)) ^ 2)))
shrink_df %>%
  ggplot(aes(shrinkage, mse_boost)) +
    geom_line() + 
    labs(title = 'Test MSE for Boosting with different Shrinkage Values',
         subtitle = '100 Trees Used in Boosting',
         x = paste('Shrinkage Value: ', expression(lambda)),
         y = 'Test MSE')
```

As we can see from the above graph, at least with our biden data, lowering the shrinkage value, $\lambda$ always results in a lower test MSE. Note however, that the lowest test MSE with the lowest value of $\lambda$ still has a higher test MSE than what we saw utilizing bagging.

# Part 2: Modeling Voter Turnout

#### 1. Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five tree-based models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)

```{r}
df.mh.10fold <- crossv_kfold(na.omit(df.mhealth), k = 10))

# Standard Decision Tree
trees.mhealth <- map(df.mh.10fold$train, ~ tree(vote96 ~ ., data = .))

# Optimally Pruned Decision Tree

# Bagged Decision Tree

# Random Forest

# Boosted Decision Tree
models.kfolds <- map(df.kfolds$train, ~ lm(biden ~ ., data = .))
mse.kfolds <- map2_dbl(models.kfolds, df.kfolds$test, calc_mse)
mse <- mean(mse.kfolds)
```